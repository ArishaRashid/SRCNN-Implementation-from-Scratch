{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Convolution Model\n",
        "VGG19 architecture\n"
      ],
      "metadata": {
        "id": "L2PqOokg3LAT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U scikit-image\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KENTTSlG8lLU",
        "outputId": "3ce1fd97-b937-477e-c209-841bd64cb6db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (0.19.3)\n",
            "Collecting scikit-image\n",
            "  Downloading scikit_image-0.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.7/14.7 MB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.8 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (1.11.4)\n",
            "Requirement already satisfied: networkx>=2.8 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (3.2.1)\n",
            "Requirement already satisfied: pillow>=9.0.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (9.4.0)\n",
            "Requirement already satisfied: imageio>=2.27 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (2.31.6)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (2023.12.9)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (23.2)\n",
            "Requirement already satisfied: lazy_loader>=0.3 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (0.3)\n",
            "Installing collected packages: scikit-image\n",
            "  Attempting uninstall: scikit-image\n",
            "    Found existing installation: scikit-image 0.19.3\n",
            "    Uninstalling scikit-image-0.19.3:\n",
            "      Successfully uninstalled scikit-image-0.19.3\n",
            "Successfully installed scikit-image-0.22.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "hws_6n8j2439"
      },
      "outputs": [],
      "source": [
        "from torchvision import models\n",
        "from collections import namedtuple\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class vgg19(nn.Module):\n",
        "\n",
        "    def __init__(self, pre_trained = True, require_grad = False):\n",
        "        super(vgg19, self).__init__()\n",
        "        self.vgg_feature = models.vgg19(pretrained = pre_trained).features\n",
        "        self.seq_list = [nn.Sequential(ele) for ele in self.vgg_feature]\n",
        "        self.vgg_layer = ['conv1_1', 'relu1_1', 'conv1_2', 'relu1_2', 'pool1',\n",
        "                         'conv2_1', 'relu2_1', 'conv2_2', 'relu2_2', 'pool2',\n",
        "                         'conv3_1', 'relu3_1', 'conv3_2', 'relu3_2', 'conv3_3', 'relu3_3', 'conv3_4', 'relu3_4', 'pool3',\n",
        "                         'conv4_1', 'relu4_1', 'conv4_2', 'relu4_2', 'conv4_3', 'relu4_3', 'conv4_4', 'relu4_4', 'pool4',\n",
        "                         'conv5_1', 'relu5_1', 'conv5_2', 'relu5_2', 'conv5_3', 'relu5_3', 'conv5_4', 'relu5_4', 'pool5']\n",
        "\n",
        "        if not require_grad:\n",
        "            for parameter in self.parameters():\n",
        "                parameter.requires_grad = False\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        conv1_1 = self.seq_list[0](x)\n",
        "        relu1_1 = self.seq_list[1](conv1_1)\n",
        "        conv1_2 = self.seq_list[2](relu1_1)\n",
        "        relu1_2 = self.seq_list[3](conv1_2)\n",
        "        pool1 = self.seq_list[4](relu1_2)\n",
        "\n",
        "        conv2_1 = self.seq_list[5](pool1)\n",
        "        relu2_1 = self.seq_list[6](conv2_1)\n",
        "        conv2_2 = self.seq_list[7](relu2_1)\n",
        "        relu2_2 = self.seq_list[8](conv2_2)\n",
        "        pool2 = self.seq_list[9](relu2_2)\n",
        "\n",
        "        conv3_1 = self.seq_list[10](pool2)\n",
        "        relu3_1 = self.seq_list[11](conv3_1)\n",
        "        conv3_2 = self.seq_list[12](relu3_1)\n",
        "        relu3_2 = self.seq_list[13](conv3_2)\n",
        "        conv3_3 = self.seq_list[14](relu3_2)\n",
        "        relu3_3 = self.seq_list[15](conv3_3)\n",
        "        conv3_4 = self.seq_list[16](relu3_3)\n",
        "        relu3_4 = self.seq_list[17](conv3_4)\n",
        "        pool3 = self.seq_list[18](relu3_4)\n",
        "\n",
        "        conv4_1 = self.seq_list[19](pool3)\n",
        "        relu4_1 = self.seq_list[20](conv4_1)\n",
        "        conv4_2 = self.seq_list[21](relu4_1)\n",
        "        relu4_2 = self.seq_list[22](conv4_2)\n",
        "        conv4_3 = self.seq_list[23](relu4_2)\n",
        "        relu4_3 = self.seq_list[24](conv4_3)\n",
        "        conv4_4 = self.seq_list[25](relu4_3)\n",
        "        relu4_4 = self.seq_list[26](conv4_4)\n",
        "        pool4 = self.seq_list[27](relu4_4)\n",
        "\n",
        "        conv5_1 = self.seq_list[28](pool4)\n",
        "        relu5_1 = self.seq_list[29](conv5_1)\n",
        "        conv5_2 = self.seq_list[30](relu5_1)\n",
        "        relu5_2 = self.seq_list[31](conv5_2)\n",
        "        conv5_3 = self.seq_list[32](relu5_2)\n",
        "        relu5_3 = self.seq_list[33](conv5_3)\n",
        "        conv5_4 = self.seq_list[34](relu5_3)\n",
        "        relu5_4 = self.seq_list[35](conv5_4)\n",
        "        pool5 = self.seq_list[36](relu5_4)\n",
        "\n",
        "        vgg_output = namedtuple(\"vgg_output\", self.vgg_layer)\n",
        "\n",
        "        vgg_list = [conv1_1, relu1_1, conv1_2, relu1_2, pool1,\n",
        "                         conv2_1, relu2_1, conv2_2, relu2_2, pool2,\n",
        "                         conv3_1, relu3_1, conv3_2, relu3_2, conv3_3, relu3_3, conv3_4, relu3_4, pool3,\n",
        "                         conv4_1, relu4_1, conv4_2, relu4_2, conv4_3, relu4_3, conv4_4, relu4_4, pool4,\n",
        "                         conv5_1, relu5_1, conv5_2, relu5_2, conv5_3, relu5_3, conv5_4, relu5_4, pool5]\n",
        "\n",
        "        out = vgg_output(*vgg_list)\n",
        "\n",
        "\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "OPs\n"
      ],
      "metadata": {
        "id": "3aoyIDBW5wuV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class _conv(nn.Conv2d):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, bias):\n",
        "        super(_conv, self).__init__(in_channels = in_channels, out_channels = out_channels,\n",
        "                               kernel_size = kernel_size, stride = stride, padding = (kernel_size) // 2, bias = True)\n",
        "\n",
        "        self.weight.data = torch.normal(torch.zeros((out_channels, in_channels, kernel_size, kernel_size)), 0.02)\n",
        "        self.bias.data = torch.zeros((out_channels))\n",
        "\n",
        "        for p in self.parameters():\n",
        "            p.requires_grad = True\n",
        "\n",
        "\n",
        "class conv(nn.Module):\n",
        "    def __init__(self, in_channel, out_channel, kernel_size, BN = False, act = None, stride = 1, bias = True):\n",
        "        super(conv, self).__init__()\n",
        "        m = []\n",
        "        m.append(_conv(in_channels = in_channel, out_channels = out_channel,\n",
        "                               kernel_size = kernel_size, stride = stride, padding = (kernel_size) // 2, bias = True))\n",
        "\n",
        "        if BN:\n",
        "            m.append(nn.BatchNorm2d(num_features = out_channel))\n",
        "\n",
        "        if act is not None:\n",
        "            m.append(act)\n",
        "\n",
        "        self.body = nn.Sequential(*m)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.body(x)\n",
        "        return out\n",
        "\n",
        "class ResBlock(nn.Module):\n",
        "    def __init__(self, channels, kernel_size, act = nn.ReLU(inplace = True), bias = True):\n",
        "        super(ResBlock, self).__init__()\n",
        "        m = []\n",
        "        m.append(conv(channels, channels, kernel_size, BN = True, act = act))\n",
        "        m.append(conv(channels, channels, kernel_size, BN = True, act = None))\n",
        "        self.body = nn.Sequential(*m)\n",
        "\n",
        "    def forward(self, x):\n",
        "        res = self.body(x)\n",
        "        res += x\n",
        "        return res\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, num_res_block, act = nn.ReLU(inplace = True)):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        m = []\n",
        "\n",
        "        self.conv = conv(in_channels, out_channels, kernel_size, BN = False, act = act)\n",
        "        for i in range(num_res_block):\n",
        "            m.append(ResBlock(out_channels, kernel_size, act))\n",
        "\n",
        "        m.append(conv(out_channels, out_channels, kernel_size, BN = True, act = None))\n",
        "\n",
        "        self.body = nn.Sequential(*m)\n",
        "\n",
        "    def forward(self, x):\n",
        "        res = self.conv(x)\n",
        "        out = self.body(res)\n",
        "        out += res\n",
        "\n",
        "        return out\n",
        "\n",
        "class Upsampler(nn.Module):\n",
        "    def __init__(self, channel, kernel_size, scale, act = nn.ReLU(inplace = True)):\n",
        "        super(Upsampler, self).__init__()\n",
        "        m = []\n",
        "        m.append(conv(channel, channel * scale * scale, kernel_size))\n",
        "        m.append(nn.PixelShuffle(scale))\n",
        "\n",
        "        if act is not None:\n",
        "            m.append(act)\n",
        "\n",
        "        self.body = nn.Sequential(*m)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.body(x)\n",
        "        return out\n",
        "\n",
        "class discrim_block(nn.Module):\n",
        "    def __init__(self, in_feats, out_feats, kernel_size, act = nn.LeakyReLU(inplace = True)):\n",
        "        super(discrim_block, self).__init__()\n",
        "        m = []\n",
        "        m.append(conv(in_feats, out_feats, kernel_size, BN = True, act = act))\n",
        "        m.append(conv(out_feats, out_feats, kernel_size, BN = True, act = act, stride = 2))\n",
        "        self.body = nn.Sequential(*m)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.body(x)\n",
        "        return out"
      ],
      "metadata": {
        "id": "1XkeQlxD5xxa"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Srgan Model"
      ],
      "metadata": {
        "id": "JBchtHlN4MSZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Generator(nn.Module):\n",
        "\n",
        "    def __init__(self, img_feat = 3, n_feats = 64, kernel_size = 3, num_block = 16, act = nn.PReLU(), scale=4):\n",
        "        super(Generator, self).__init__()\n",
        "\n",
        "        self.conv01 = conv(in_channel = img_feat, out_channel = n_feats, kernel_size = 9, BN = False, act = act)\n",
        "\n",
        "        resblocks = [ResBlock(channels = n_feats, kernel_size = 3, act = act) for _ in range(num_block)]\n",
        "        self.body = nn.Sequential(*resblocks)\n",
        "\n",
        "        self.conv02 = conv(in_channel = n_feats, out_channel = n_feats, kernel_size = 3, BN = True, act = None)\n",
        "\n",
        "        if(scale == 4):\n",
        "            upsample_blocks = [Upsampler(channel = n_feats, kernel_size = 3, scale = 2, act = act) for _ in range(2)]\n",
        "        else:\n",
        "            upsample_blocks = [Upsampler(channel = n_feats, kernel_size = 3, scale = scale, act = act)]\n",
        "\n",
        "        self.tail = nn.Sequential(*upsample_blocks)\n",
        "\n",
        "        self.last_conv = conv(in_channel = n_feats, out_channel = img_feat, kernel_size = 3, BN = False, act = nn.Tanh())\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = self.conv01(x)\n",
        "        _skip_connection = x\n",
        "\n",
        "        x = self.body(x)\n",
        "        x = self.conv02(x)\n",
        "        feat = x + _skip_connection\n",
        "\n",
        "        x = self.tail(feat)\n",
        "        x = self.last_conv(x)\n",
        "\n",
        "        return x, feat\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "\n",
        "    def __init__(self, img_feat = 3, n_feats = 64, kernel_size = 3, act = nn.LeakyReLU(inplace = True), num_of_block = 3, patch_size = 96):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.act = act\n",
        "\n",
        "        self.conv01 = conv(in_channel = img_feat, out_channel = n_feats, kernel_size = 3, BN = False, act = self.act)\n",
        "        self.conv02 = conv(in_channel = n_feats, out_channel = n_feats, kernel_size = 3, BN = False, act = self.act, stride = 2)\n",
        "\n",
        "        body = [discrim_block(in_feats = n_feats * (2 ** i), out_feats = n_feats * (2 ** (i + 1)), kernel_size = 3, act = self.act) for i in range(num_of_block)]\n",
        "        self.body = nn.Sequential(*body)\n",
        "\n",
        "        self.linear_size = ((patch_size // (2 ** (num_of_block + 1))) ** 2) * (n_feats * (2 ** num_of_block))\n",
        "\n",
        "        tail = []\n",
        "\n",
        "        tail.append(nn.Linear(self.linear_size, 1024))\n",
        "        tail.append(self.act)\n",
        "        tail.append(nn.Linear(1024, 1))\n",
        "        tail.append(nn.Sigmoid())\n",
        "\n",
        "        self.tail = nn.Sequential(*tail)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = self.conv01(x)\n",
        "        x = self.conv02(x)\n",
        "        x = self.body(x)\n",
        "        x = x.view(-1, self.linear_size)\n",
        "        x = self.tail(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "f9oljvpq4Mql"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "# from your_module import mydata, testOnly_data, Generator, Discriminator, vgg19, perceptual_loss, TVLoss, crop, augmentation  # Import your modules and functions\n",
        "\n",
        "def train(LR_path, GT_path, res_num=16, num_workers=0, batch_size=16, L2_coeff=1.0, adv_coeff=1e-3,\n",
        "          tv_loss_coeff=0.0, pre_train_epoch=8000, fine_train_epoch=4000, scale=4, patch_size=24,\n",
        "          feat_layer='relu5_4', vgg_rescale_coeff=0.006, fine_tuning=False, in_memory=True, generator_path=None):\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    transform = transforms.Compose([crop(scale, patch_size), augmentation()])\n",
        "    dataset = mydata(GT_path=GT_path, LR_path=LR_path, in_memory=in_memory, transform=transform)\n",
        "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
        "\n",
        "    generator = Generator(img_feat=3, n_feats=64, kernel_size=3, num_block=res_num, scale=scale)\n",
        "\n",
        "    if fine_tuning:\n",
        "        generator.load_state_dict(torch.load(generator_path))\n",
        "        print(\"pre-trained model is loaded\")\n",
        "        print(\"path : %s\" % (generator_path))\n",
        "\n",
        "    generator = generator.to(device)\n",
        "    generator.train()\n",
        "\n",
        "    l2_loss = nn.MSELoss()\n",
        "    g_optim = optim.Adam(generator.parameters(), lr=1e-4)\n",
        "\n",
        "    pre_epoch = 0\n",
        "    fine_epoch = 0\n",
        "\n",
        "    # Train using L2_loss\n",
        "    while pre_epoch < pre_train_epoch:\n",
        "        for i, tr_data in enumerate(loader):\n",
        "            gt = tr_data['GT'].to(device)\n",
        "            lr = tr_data['LR'].to(device)\n",
        "\n",
        "            output, _ = generator(lr)\n",
        "            loss = l2_loss(gt, output)\n",
        "\n",
        "            g_optim.zero_grad()\n",
        "            loss.backward()\n",
        "            g_optim.step()\n",
        "\n",
        "        pre_epoch += 1\n",
        "\n",
        "        if pre_epoch % 2 == 0:\n",
        "            print(pre_epoch)\n",
        "            print(loss.item())\n",
        "            print('=========')\n",
        "\n",
        "        if pre_epoch % 800 == 0:\n",
        "            torch.save(generator.state_dict(), '/content/drive/MyDrive/model/pre_trained_model_%03d.pt' % pre_epoch)\n",
        "\n",
        "    # Train using perceptual & adversarial loss\n",
        "    vgg_net = vgg19().to(device)\n",
        "    vgg_net = vgg_net.eval()\n",
        "\n",
        "    discriminator = Discriminator(patch_size=patch_size * scale)\n",
        "    discriminator = discriminator.to(device)\n",
        "    discriminator.train()\n",
        "\n",
        "    d_optim = optim.Adam(discriminator.parameters(), lr=1e-4)\n",
        "    scheduler = optim.lr_scheduler.StepLR(g_optim, step_size=2000, gamma=0.1)\n",
        "\n",
        "    VGG_loss = perceptual_loss(vgg_net)\n",
        "    cross_ent = nn.BCELoss()\n",
        "    tv_loss = TVLoss()\n",
        "    real_label = torch.ones((batch_size, 1)).to(device)\n",
        "    fake_label = torch.zeros((batch_size, 1)).to(device)\n",
        "\n",
        "    while fine_epoch < fine_train_epoch:\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        for i, tr_data in enumerate(loader):\n",
        "            gt = tr_data['GT'].to(device)\n",
        "            lr = tr_data['LR'].to(device)\n",
        "\n",
        "            # Training Discriminator\n",
        "            output, _ = generator(lr)\n",
        "            fake_prob = discriminator(output)\n",
        "            real_prob = discriminator(gt)\n",
        "\n",
        "            d_loss_real = cross_ent(real_prob, real_label)\n",
        "            d_loss_fake = cross_ent(fake_prob, fake_label)\n",
        "\n",
        "            d_loss = d_loss_real + d_loss_fake\n",
        "\n",
        "            g_optim.zero_grad()\n",
        "            d_optim.zero_grad()\n",
        "            d_loss.backward()\n",
        "            d_optim.step()\n",
        "\n",
        "            # Training Generator\n",
        "            output, _ = generator(lr)\n",
        "            fake_prob = discriminator(output)\n",
        "\n",
        "            _percep_loss, hr_feat, sr_feat = VGG_loss((gt + 1.0) / 2.0, (output + 1.0) / 2.0, layer=feat_layer)\n",
        "\n",
        "            L2_loss = l2_loss(output, gt)\n",
        "            percep_loss = vgg_rescale_coeff * _percep_loss\n",
        "            adversarial_loss = adv_coeff * cross_ent(fake_prob, real_label)\n",
        "            total_variance_loss = tv_loss(tv_loss_coeff * (hr_feat - sr_feat) ** 2)\n",
        "\n",
        "            g_loss = percep_loss + adversarial_loss + total_variance_loss + L2_loss\n",
        "\n",
        "            g_optim.zero_grad()\n",
        "            d_optim.zero_grad()\n",
        "            g_loss.backward()\n",
        "            g_optim.step()\n",
        "\n",
        "        fine_epoch += 1\n",
        "\n",
        "        if fine_epoch % 2 == 0:\n",
        "            print(fine_epoch)\n",
        "            print(g_loss.item())\n",
        "            print(d_loss.item())\n",
        "            print('=========')\n",
        "\n",
        "        if fine_epoch % 500 == 0:\n",
        "            torch.save(generator.state_dict(), '/content/drive/MyDrive/model/SRGAN_gene_%03d.pt' % fine_epoch)\n",
        "            torch.save(discriminator.state_dict(), '/content/drive/MyDrive/model/SRGAN_discrim_%03d.pt' % fine_epoch)\n",
        "\n",
        "\n",
        "def test_only(LR_path, num_workers=0, generator_path=None):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    dataset = testOnly_data(LR_path=LR_path, in_memory=False, transform=None)\n",
        "    loader = DataLoader(dataset, batch_size=1, shuffle=False, num_workers=num_workers)\n",
        "\n",
        "    generator = Generator(img_feat=3, n_feats=64, kernel_size=3, num_block=16)\n",
        "    generator.load_state_dict(torch.load(generator_path))\n",
        "    generator = generator.to(device)\n",
        "    generator.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, te_data in enumerate(loader):\n",
        "            lr = te_data['LR'].to(device)\n",
        "            output, _ = generator(lr)\n",
        "            output = output[0].cpu().numpy()\n",
        "            output = (output + 1.0) / 2.0\n",
        "            output = output.transpose(1, 2, 0)\n",
        "            result = Image.fromarray((output * 255.0).astype(np.uint8))\n",
        "            result.save('/content/drive/MyDrive/result/res_%04d.png' % i)\n"
      ],
      "metadata": {
        "id": "2KoGDgTs3Q1U"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculating Losses"
      ],
      "metadata": {
        "id": "Ulwq0Y4kFawF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import transforms\n",
        "\n",
        "\n",
        "class MeanShift(nn.Conv2d):\n",
        "    def __init__(\n",
        "        self, rgb_range = 1,\n",
        "        norm_mean=(0.485, 0.456, 0.406), norm_std=(0.229, 0.224, 0.225), sign=-1):\n",
        "\n",
        "        super(MeanShift, self).__init__(3, 3, kernel_size=1)\n",
        "        std = torch.Tensor(norm_std)\n",
        "        self.weight.data = torch.eye(3).view(3, 3, 1, 1) / std.view(3, 1, 1, 1)\n",
        "        self.bias.data = sign * rgb_range * torch.Tensor(norm_mean) / std\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        for p in self.parameters():\n",
        "            p.requires_grad = False\n",
        "\n",
        "\n",
        "class perceptual_loss(nn.Module):\n",
        "\n",
        "    def __init__(self, vgg):\n",
        "        super(perceptual_loss, self).__init__()\n",
        "        self.normalization_mean = [0.485, 0.456, 0.406]\n",
        "        self.normalization_std = [0.229, 0.224, 0.225]\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.transform = MeanShift(norm_mean = self.normalization_mean, norm_std = self.normalization_std).to(self.device)\n",
        "        self.vgg = vgg\n",
        "        self.criterion = nn.MSELoss()\n",
        "    def forward(self, HR, SR, layer = 'relu5_4'):\n",
        "        ## HR and SR should be normalized [0,1]\n",
        "        hr = self.transform(HR)\n",
        "        sr = self.transform(SR)\n",
        "\n",
        "        hr_feat = getattr(self.vgg(hr), layer)\n",
        "        sr_feat = getattr(self.vgg(sr), layer)\n",
        "\n",
        "        return self.criterion(hr_feat, sr_feat), hr_feat, sr_feat\n",
        "\n",
        "class TVLoss(nn.Module):\n",
        "    def __init__(self, tv_loss_weight=1):\n",
        "        super(TVLoss, self).__init__()\n",
        "        self.tv_loss_weight = tv_loss_weight\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.size()[0]\n",
        "        h_x = x.size()[2]\n",
        "        w_x = x.size()[3]\n",
        "        count_h = self.tensor_size(x[:, :, 1:, :])\n",
        "        count_w = self.tensor_size(x[:, :, :, 1:])\n",
        "        h_tv = torch.pow((x[:, :, 1:, :] - x[:, :, :h_x - 1, :]), 2).sum()\n",
        "        w_tv = torch.pow((x[:, :, :, 1:] - x[:, :, :, :w_x - 1]), 2).sum()\n",
        "\n",
        "        return self.tv_loss_weight * 2 * (h_tv / count_h + w_tv / count_w) / batch_size\n",
        "\n",
        "    @staticmethod\n",
        "    def tensor_size(t):\n",
        "        return t.size()[1] * t.size()[2] * t.size()[3]"
      ],
      "metadata": {
        "id": "tdzP1RvcFYkO"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "m8_IScIUHmWK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import os\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "\n",
        "class mydata(Dataset):\n",
        "    def __init__(self, LR_path, GT_path, in_memory = True, transform = None):\n",
        "\n",
        "        self.LR_path = LR_path\n",
        "        self.GT_path = GT_path\n",
        "        self.in_memory = in_memory\n",
        "        self.transform = transform\n",
        "\n",
        "        self.LR_img = sorted(os.listdir(LR_path))\n",
        "        self.GT_img = sorted(os.listdir(GT_path))\n",
        "\n",
        "        if in_memory:\n",
        "            self.LR_img = [np.array(Image.open(os.path.join(self.LR_path, lr)).convert(\"RGB\")).astype(np.uint8) for lr in self.LR_img]\n",
        "            self.GT_img = [np.array(Image.open(os.path.join(self.GT_path, gt)).convert(\"RGB\")).astype(np.uint8) for gt in self.GT_img]\n",
        "\n",
        "    def __len__(self):\n",
        "\n",
        "        return len(self.LR_img)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "\n",
        "        img_item = {}\n",
        "\n",
        "        if self.in_memory:\n",
        "            GT = self.GT_img[i].astype(np.float32)\n",
        "            LR = self.LR_img[i].astype(np.float32)\n",
        "\n",
        "        else:\n",
        "            GT = np.array(Image.open(os.path.join(self.GT_path, self.GT_img[i])).convert(\"RGB\"))\n",
        "            LR = np.array(Image.open(os.path.join(self.LR_path, self.LR_img[i])).convert(\"RGB\"))\n",
        "\n",
        "        img_item['GT'] = (GT / 127.5) - 1.0\n",
        "        img_item['LR'] = (LR / 127.5) - 1.0\n",
        "\n",
        "        if self.transform is not None:\n",
        "            img_item = self.transform(img_item)\n",
        "\n",
        "        img_item['GT'] = img_item['GT'].transpose(2, 0, 1).astype(np.float32)\n",
        "        img_item['LR'] = img_item['LR'].transpose(2, 0, 1).astype(np.float32)\n",
        "\n",
        "        return img_item\n",
        "\n",
        "\n",
        "class testOnly_data(Dataset):\n",
        "    def __init__(self, LR_path, in_memory = True, transform = None):\n",
        "\n",
        "        self.LR_path = LR_path\n",
        "        self.LR_img = sorted(os.listdir(LR_path))\n",
        "        self.in_memory = in_memory\n",
        "        if in_memory:\n",
        "            self.LR_img = [np.array(Image.open(os.path.join(self.LR_path, lr))) for lr in self.LR_img]\n",
        "\n",
        "    def __len__(self):\n",
        "\n",
        "        return len(self.LR_img)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "\n",
        "        img_item = {}\n",
        "\n",
        "        if self.in_memory:\n",
        "            LR = self.LR_img[i]\n",
        "\n",
        "        else:\n",
        "            LR = np.array(Image.open(os.path.join(self.LR_path, self.LR_img[i])))\n",
        "\n",
        "        img_item['LR'] = (LR / 127.5) - 1.0\n",
        "        img_item['LR'] = img_item['LR'].transpose(2, 0, 1).astype(np.float32)\n",
        "\n",
        "        return img_item\n",
        "\n",
        "\n",
        "class crop(object):\n",
        "    def __init__(self, scale, patch_size):\n",
        "\n",
        "        self.scale = scale\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        LR_img, GT_img = sample['LR'], sample['GT']\n",
        "        ih, iw = LR_img.shape[:2]\n",
        "\n",
        "        ix = random.randrange(0, iw - self.patch_size +1)\n",
        "        iy = random.randrange(0, ih - self.patch_size +1)\n",
        "\n",
        "        tx = ix * self.scale\n",
        "        ty = iy * self.scale\n",
        "\n",
        "        LR_patch = LR_img[iy : iy + self.patch_size, ix : ix + self.patch_size]\n",
        "        GT_patch = GT_img[ty : ty + (self.scale * self.patch_size), tx : tx + (self.scale * self.patch_size)]\n",
        "\n",
        "        return {'LR' : LR_patch, 'GT' : GT_patch}\n",
        "\n",
        "class augmentation(object):\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        LR_img, GT_img = sample['LR'], sample['GT']\n",
        "\n",
        "        hor_flip = random.randrange(0,2)\n",
        "        ver_flip = random.randrange(0,2)\n",
        "        rot = random.randrange(0,2)\n",
        "\n",
        "        if hor_flip:\n",
        "            temp_LR = np.fliplr(LR_img)\n",
        "            LR_img = temp_LR.copy()\n",
        "            temp_GT = np.fliplr(GT_img)\n",
        "            GT_img = temp_GT.copy()\n",
        "\n",
        "            del temp_LR, temp_GT\n",
        "\n",
        "        if ver_flip:\n",
        "            temp_LR = np.flipud(LR_img)\n",
        "            LR_img = temp_LR.copy()\n",
        "            temp_GT = np.flipud(GT_img)\n",
        "            GT_img = temp_GT.copy()\n",
        "\n",
        "            del temp_LR, temp_GT\n",
        "\n",
        "        if rot:\n",
        "            LR_img = LR_img.transpose(1, 0, 2)\n",
        "            GT_img = GT_img.transpose(1, 0, 2)\n",
        "\n",
        "\n",
        "        return {'LR' : LR_img, 'GT' : GT_img}"
      ],
      "metadata": {
        "id": "a1W94DI5HmjW"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Demo"
      ],
      "metadata": {
        "id": "CSdc4kpxHOtK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train(LR_path='/content/drive/MyDrive/LR', GT_path='/content/drive/MyDrive/HR_1')\n",
        "# test_only(LR_path='/content/drive/MyDrive/LR', generator_path='/content/drive/MyDrive/model/SRGAN_gene_1000.pt')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rX5un01J8XzU",
        "outputId": "5ba7019f-eafb-4462-cf5c-c7e802e3dd9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n",
            "0.07081858813762665\n",
            "=========\n",
            "4\n",
            "0.04647092893719673\n",
            "=========\n",
            "6\n",
            "0.04484109953045845\n",
            "=========\n",
            "8\n",
            "0.0604960136115551\n",
            "=========\n",
            "10\n",
            "0.037511274218559265\n",
            "=========\n",
            "12\n",
            "0.034894976764917374\n",
            "=========\n",
            "14\n",
            "0.037066489458084106\n",
            "=========\n",
            "16\n",
            "0.03790496662259102\n",
            "=========\n",
            "18\n",
            "0.026696303859353065\n",
            "=========\n",
            "20\n",
            "0.04488465562462807\n",
            "=========\n",
            "22\n",
            "0.03856632485985756\n",
            "=========\n",
            "24\n",
            "0.03148430585861206\n",
            "=========\n",
            "26\n",
            "0.026282135397195816\n",
            "=========\n",
            "28\n",
            "0.03503815084695816\n",
            "=========\n",
            "30\n",
            "0.025875110179185867\n",
            "=========\n",
            "32\n",
            "0.028284041211009026\n",
            "=========\n",
            "34\n",
            "0.027127256616950035\n",
            "=========\n",
            "36\n",
            "0.03465304151177406\n",
            "=========\n",
            "38\n",
            "0.02902963198721409\n",
            "=========\n",
            "40\n",
            "0.03159463033080101\n",
            "=========\n",
            "42\n",
            "0.02861742675304413\n",
            "=========\n",
            "44\n",
            "0.01917005144059658\n",
            "=========\n",
            "46\n",
            "0.02129826322197914\n",
            "=========\n",
            "48\n",
            "0.03671082854270935\n",
            "=========\n",
            "50\n",
            "0.03970620036125183\n",
            "=========\n",
            "52\n",
            "0.0320647694170475\n",
            "=========\n",
            "54\n",
            "0.02955683320760727\n",
            "=========\n",
            "56\n",
            "0.02164018526673317\n",
            "=========\n",
            "58\n",
            "0.023546740412712097\n",
            "=========\n",
            "60\n",
            "0.03282942622900009\n",
            "=========\n",
            "62\n",
            "0.027837160974740982\n",
            "=========\n",
            "64\n",
            "0.02902170456945896\n",
            "=========\n",
            "66\n",
            "0.023813428357243538\n",
            "=========\n",
            "68\n",
            "0.024580053985118866\n",
            "=========\n",
            "70\n",
            "0.023071281611919403\n",
            "=========\n",
            "72\n",
            "0.028537457808852196\n",
            "=========\n",
            "74\n",
            "0.030653702095150948\n",
            "=========\n",
            "76\n",
            "0.022461937740445137\n",
            "=========\n",
            "78\n",
            "0.023115117102861404\n",
            "=========\n",
            "80\n",
            "0.026557253673672676\n",
            "=========\n",
            "82\n",
            "0.02812751568853855\n",
            "=========\n",
            "84\n",
            "0.02196364477276802\n",
            "=========\n",
            "86\n",
            "0.02314835786819458\n",
            "=========\n",
            "88\n",
            "0.028895827010273933\n",
            "=========\n",
            "90\n",
            "0.02057485282421112\n",
            "=========\n",
            "92\n",
            "0.028336482122540474\n",
            "=========\n",
            "94\n",
            "0.034208033233881\n",
            "=========\n",
            "96\n",
            "0.0212840773165226\n",
            "=========\n",
            "98\n",
            "0.023363253101706505\n",
            "=========\n",
            "100\n",
            "0.02687653712928295\n",
            "=========\n",
            "102\n",
            "0.022868821397423744\n",
            "=========\n",
            "104\n",
            "0.030187083408236504\n",
            "=========\n",
            "106\n",
            "0.024137187749147415\n",
            "=========\n",
            "108\n",
            "0.023278772830963135\n",
            "=========\n",
            "110\n",
            "0.024868996813893318\n",
            "=========\n",
            "112\n",
            "0.028739256784319878\n",
            "=========\n",
            "114\n",
            "0.02363486960530281\n",
            "=========\n",
            "116\n",
            "0.023086193948984146\n",
            "=========\n",
            "118\n",
            "0.022400852292776108\n",
            "=========\n",
            "120\n",
            "0.0226292721927166\n",
            "=========\n",
            "122\n",
            "0.024202091619372368\n",
            "=========\n",
            "124\n",
            "0.020021669566631317\n",
            "=========\n",
            "126\n",
            "0.030409008264541626\n",
            "=========\n",
            "128\n",
            "0.027333861216902733\n",
            "=========\n",
            "130\n",
            "0.019807348027825356\n",
            "=========\n",
            "132\n",
            "0.02696496620774269\n",
            "=========\n",
            "134\n",
            "0.02514074184000492\n",
            "=========\n",
            "136\n",
            "0.02604399248957634\n",
            "=========\n",
            "138\n",
            "0.024445513263344765\n",
            "=========\n",
            "140\n",
            "0.023938322439789772\n",
            "=========\n",
            "142\n",
            "0.029761232435703278\n",
            "=========\n",
            "144\n",
            "0.029495935887098312\n",
            "=========\n",
            "146\n",
            "0.025215953588485718\n",
            "=========\n",
            "148\n",
            "0.02205602452158928\n",
            "=========\n",
            "150\n",
            "0.028600236400961876\n",
            "=========\n",
            "152\n",
            "0.030231744050979614\n",
            "=========\n",
            "154\n",
            "0.02477850951254368\n",
            "=========\n",
            "156\n",
            "0.024979455396533012\n",
            "=========\n",
            "158\n",
            "0.020638123154640198\n",
            "=========\n",
            "160\n",
            "0.02723252773284912\n",
            "=========\n",
            "162\n",
            "0.02975216694176197\n",
            "=========\n",
            "164\n",
            "0.027084622532129288\n",
            "=========\n",
            "166\n",
            "0.02830863744020462\n",
            "=========\n",
            "168\n",
            "0.02914407290518284\n",
            "=========\n",
            "170\n",
            "0.028390346094965935\n",
            "=========\n",
            "172\n",
            "0.018428806215524673\n",
            "=========\n",
            "174\n",
            "0.02455364726483822\n",
            "=========\n",
            "176\n",
            "0.02265382744371891\n",
            "=========\n",
            "178\n",
            "0.03432141989469528\n",
            "=========\n",
            "180\n",
            "0.02231498621404171\n",
            "=========\n",
            "182\n",
            "0.024964040145277977\n",
            "=========\n",
            "184\n",
            "0.020169105380773544\n",
            "=========\n",
            "186\n",
            "0.026755155995488167\n",
            "=========\n",
            "188\n",
            "0.03110845759510994\n",
            "=========\n",
            "190\n",
            "0.02078663371503353\n",
            "=========\n",
            "192\n",
            "0.024402983486652374\n",
            "=========\n",
            "194\n",
            "0.020790159702301025\n",
            "=========\n",
            "196\n",
            "0.021517211571335793\n",
            "=========\n",
            "198\n",
            "0.026754483580589294\n",
            "=========\n",
            "200\n",
            "0.02245211787521839\n",
            "=========\n",
            "202\n",
            "0.017922285944223404\n",
            "=========\n",
            "204\n",
            "0.022144844755530357\n",
            "=========\n",
            "206\n",
            "0.021053999662399292\n",
            "=========\n",
            "208\n",
            "0.020965509116649628\n",
            "=========\n",
            "210\n",
            "0.023533977568149567\n",
            "=========\n",
            "212\n",
            "0.020538900047540665\n",
            "=========\n",
            "214\n",
            "0.027087558060884476\n",
            "=========\n",
            "216\n",
            "0.029648635536432266\n",
            "=========\n",
            "218\n",
            "0.02522105909883976\n",
            "=========\n",
            "220\n",
            "0.020504560321569443\n",
            "=========\n",
            "222\n",
            "0.020463326945900917\n",
            "=========\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "tnxLB8XcelMb"
      }
    }
  ]
}